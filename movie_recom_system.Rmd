---
title: "Movie Recommendation System"
subtitle: "HarvardX PH125.9x"
author: "Ruben Campos"
date: "25/09/22"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    latex_engine: xelatex
mainfont: "Calibri"
urlcolor: blue
---

```{r Setup, echo=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE , warning = FALSE, message = FALSE, fig.align="center", out.width="80%")

################## Install Basic Packages
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(rmarkdown)) install.packages("rmarkdown", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")

################## Additional Packages
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(formattable)) install.packages("formattable", repos = "http://cran.us.r-project.org")
if(!require(psych)) install.packages("psych", repos = "http://cran.us.r-project.org")
```

```{r Loading_libraries, message=FALSE, warning=FALSE, include=FALSE}
library(caret)
library(data.table)
library(dplyr)
library(formattable)
library(ggthemes)
library(kableExtra)
library(knitr)
library(lubridate)
library(psych)
library(recosystem)
library(rmarkdown)
library(tidyverse)

```

```{r Format_Functions, warning=FALSE, include=FALSE}

#Set thousands separator
knitr::knit_hooks$set(inline = function(x) { if(!is.numeric(x)){ x }else{ prettyNum(round(x,2), big.mark=".") } })

#Setting a higher resolution for output plots. 
knitr::opts_chunk$set(dpi=300)

#Function for thousands separator in table output
niceKable = function(...) {
  knitr::kable(..., 
               position = 'HOLD_position',
               format.args = list(decimal.mark = ',',
               big.mark = ".")) %>% kable_styling(latex_options = "HOLD_position")
}

```

\newpage
# 1.	Introduction

A recommendation system is an information filtering system that tries to predict the preference that a user would give to an item that want to consume in some way: be it clothes, electronics or entertainment. In the exercise that arises, it is about movies.

Similar systems, obviously much better in terms of complexity and accuracy, have been used for years in the entertainment industry. It is obvious that Netflix was practically the pioneer in this aspect, but other companies already used this system for the articles offered to the public, such as Amazon. In the end, the underlying idea of these systems is to always offer the customer what they think they will want, leaving little room for them to choose other things that are not in their interest.

We are going to work, in order to develop our own recommendation system, with the data provide in *MovieLens dataset*. It was first released in 1.998 by a team from the University of Minnesota, where collected the data that corresponds to thousands of people interactions with an online movie-recommendation system, a process in which the users are required to input their movie preferences into the system.

The success metric of our system will be the *Root Mean Square Estimate (RMSE)*. We will try to be able to determine movie ratings on a scale of 0.5 to 5 stars, measuring the RMSE on the same scale. It should be noted that a RMSE trending at 0 indicates that we are on the right track.

The target of this project is to achieve a RMSE < 0.87750

```{r Initial_Setup, warning=FALSE, include=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
# movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                            title = as.character(title),
#                                            genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

# 2.	 Project methodology  

The process we have followed to develop this project is as follows:  

• Download the data set.  
• Split it into two different ones: edx and validation.  
• Explore the data. Wrangling if necessary.  
• Modeling concept.  
• Evaluate the effectiveness via RMSE by splitting edx dataset into edx_train and edx_test, creating different models and cheking performance using edx_test.  
• Retraining best performance model and validation.  

# 3.	 Data source overview  

## 3.1 The data sets  

```{r Check_Data, echo=FALSE, warning=FALSE, include=FALSE}

class(edx)
dim(edx)[1]
dim(validation)[1]

dim(edx)[2]
str(edx)

# Unique values for user id, movie ids, titles, genres
n_distinct(edx$userId)
n_distinct(edx$movieId) 
n_distinct(edx$title)
n_distinct(edx$genres)

# Extract unique genres
genres <- str_extract_all(unique(edx$genres), "[^|]+") %>%
  unlist() %>%
  unique()

n_distinct(genres) # Unique genres
n_distinct(edx$genres) # Unique genres combinations
```
Movielens dataset is split in two:  

  •	edx dataset for model training.    
  •	validation dataset for model evaluation.  

The edx dataset have a total amount of `r prettyNum(nrow(edx), big.mark=".")` and `r ncol(edx)` variables (`r names(edx) %>% str_c("*", . , "*") %>% str_flatten(collapse=", ")`) corresponding to each one of the columns in the data set. We have to bear in mind each record corresponds to a rating of *one movie* by *one user*. In the other hand, validation dataset consists of `r nrow(validation)` records and the same variables.  

As mentioned, edx dataset have six columns. Those are what must be considered the predictor variables:  
  
  • userId  
  • movieId   
  • timestamp   
  • title   
  • genres  

While column rating is the outcome, as is the result that we want to predict. Following, we show a sample corresponding to edx dataset records structure:  

```{r Head_Sample, message=FALSE, warning=FALSE, echo=FALSE}

head(edx) %>% kable(booktabs = T, caption = "Sample of records structure in edx dataset") %>% kable_styling(latex_options = "HOLD_position")

```
The edx dataset predictors *userId* and *movieId* are unique identifiers corresponding to each user and movie respectively; *timestamp* applies to the case a specific user has rated a specific movie; *title* includes each movie´s title and the year that was released; *genres* are the films genre they can be associated. 

\newpage
## 3.2 Data exploration  
  
### 3.2.1	Movies & Ratings

We consider that movies and ratings should be analyzed jointly, since these parameters are intimately related and are completely dependent on. And although significant data is obtained separately, we have not seen that they are conclusive, contrary to what we are going to see next, when doing it together.

The edx dataset has a total `r format(n_distinct(edx$movieId),scientific=F, big.mark=".")` different movies, while validation dataset has `r format(n_distinct(validation$movieId),scientific=F, big.mark=".")`. 

The rating is a number provided by users and is between 0.5 (minimum, worst rating) and 5 (maximum, best rating). First step is to see the distribution of ratings (and mean rating, dashed) in order to define the mean value of the valuation on the ratings, allowing us to have a first perspective on the positivity or negativity of ratings:
```{r Ratings_Distr, echo=FALSE}

mean_rating <- mean(edx$rating)

edx %>%
ggplot(aes(x= rating)) +
   geom_histogram(binwidth=0.1, bins = 30, fill = "steelblue",  colour="black", position = "dodge") + scale_x_continuous(breaks=seq(0, 5, by= 0.5)) +
  labs(title="Ratings distribution", x="Ratings distribution (and mean, dashed)", y="Number of ratings")+ theme(plot.title=element_text(hjust=0.5))+
  geom_vline(xintercept = mean_rating, linetype="dashed",  colour = "orange", size=1)
```
Our first conclusions by analyzing the info provided from this figure, are:   
  
  •	Overall average rating at edx dataset is `r round(mean(edx$rating), 2)` It can be considered a positive rating value.  
  •	Top 3 ratings from users are (from smallest amount of ratings to largest) :  5, 4, 3. All three values are positive.  
  • Bearing in mind both points above, we assert users tend to value positively the films.  

Now we know the number of ratings and mean rating for all movies at edx dataset. But, what are the ratings the users given to all movies?. It is important to know the size of the data sample, in order to have an unbiased view of it. 

We can obtain the answer with the following figures:
```{r Ratings_Movie, echo=FALSE}

edx %>% group_by(movieId) %>%
  summarize(num_movie_rating = n(), 
            mu_movies = mean(rating),
            sd_movies = sd(rating)) %>% ggplot(aes(x = num_movie_rating))+
  geom_histogram(binwidth=0.1, bins = 30, fill = "steelblue",  colour="black", position = "dodge")+  scale_x_log10()+
  labs(title=" Number of ratings by movie ", x=" Number of ratings ", y="Number of movies")+ theme(plot.title=element_text(hjust=0.5))+
  geom_vline(aes(xintercept = mean(num_movie_rating)), linetype="dashed",  colour = "orange", size=1)

```

```{r Movies distribion, message=FALSE, echo=FALSE}

edx %>% group_by(movieId) %>%
  summarise(movie_ave_rating = sum(rating)/n()) %>%
  ggplot(aes(movie_ave_rating)) +
  geom_histogram(binwidth=0.1, bins = 30, fill = "steelblue",  colour="black", position = "dodge") +
  scale_x_continuous(breaks=seq(0, 5, by= 0.5)) + labs(title=" Movies distribution by average rating ", x=" Average rating (movies) ", y="Number of movies")+ theme(plot.title=element_text(hjust=0.5))

```
Now we´re able to say, that there´s a clear fact: some movies are more often rated than others, as we can see by the distribution of the counts of ratings, reinforcing part of what we affirmed in the analysis of the ratings distribution. But furthermore, means that movies with a higher rating average, are also rated more often:

```{r Average Rating Movies, message=FALSE, echo=FALSE}

avrm <- setDT(rbind.data.frame(edx, validation)) %>% group_by(movieId) %>% summarise(n=n(), mean_r=mean(rating)) 

avrm %>% 
  ggplot(aes(x=mean_r, y=n)) + geom_point(shape=21, fill="steelblue", color="black", size=2, alpha = 2/5) + labs(title=" Average rating of movies ", x = "Average rating per movie", y = "Number of ratings (per movie)") + geom_smooth(method = 'lm', col = 'orange') + theme(plot.title=element_text(hjust=0.5)) + scale_y_continuous()
```
By analyzing the info provided, we conclude in relation with Movies & Ratings:

  • There is a clear tendency to value positively. We believe that it is due to a replica effect: users tend to assess according to what others have previously done.  
  • This trend could be due substantially to the fact that MovieLens system offers higher rated movies more often, so users tend to rate them higher accordingly, something that may make sense if we take into account that users tend to look for well-rated movies to entertain themselves.  
  • Approximate 20% number of movies have a number of ratings higher than the average, which represents about 85% of ratings.  
  
### 3.2.2	Users  
  
```{r Users, echo=FALSE, include=FALSE}

ed <- edx %>% group_by(userId) %>% 
  summarise(ratings_per_user = n(), mean_rating_per_user = mean(rating), sd_rating_per_user = sd(rating)) 

val <- validation %>% group_by(userId) %>% 
  summarise(ratings_per_user = n(), mean_rating_per_user = mean(rating), sd_rating_per_user = sd(rating))

desc_val <- describe(val)
desc_ed <- describe(ed)
```
  
The total number of unique users in the edx dataset is `r (n_distinct(edx$userId))` , while in validation dataset is `r (n_distinct(validation$userId))` users represented by *userid*. By checking other data related with users in our datasets, we find out:  
  • in *edx dataset* the minimum number of ratings by a single user is `r desc_ed["ratings_per_user",]$min`; the maximum number is `r desc_ed["ratings_per_user",]$max`.  
  • in the *validation dataset* the minimum number of ratings by a single user is `r desc_val["ratings_per_user",]$min`, while the maximum number is `r desc_val["ratings_per_user",]$max`.   

But the mean rating each user provides to a film, is identical in both data sets: `r format(round(desc_ed["mean_rating_per_user",]$mean, 2), nsmall = 2)` 

The following histogram represents the distribution and mean of number ratings by user: 
```{r Users_Rat, message=FALSE, echo=FALSE}

edx %>% group_by(userId) %>%
  summarize(num_user_rating = n(),
            mu_user = mean(rating),
            sd_user = sd(rating)) %>% 
  ggplot(aes(x = num_user_rating))+
  geom_histogram(binwidth=0.1, bins = 30, fill = "steelblue",  colour="black", position = "dodge")+ scale_x_log10()+
  ggtitle(" Distribution and mean number ratings by users ") +
  labs(title=" Distribution and mean number ratings by users ", x=" Number of users ratings (mean dashed) ", y="Number of ratings")+ theme(plot.title=element_text(hjust=0.5))+
  geom_vline(aes(xintercept = mean(num_user_rating)), linetype="dashed",  colour = "orange", size=1)

```
  
We need to know how users rate (low, high...). We need to know the distribution by average rating: 
  
```{r Users_Avg, echo=FALSE}

edx %>% group_by(userId) %>%
  summarise(user_ave_rating = sum(rating)/n()) %>%
  ggplot(aes(user_ave_rating)) +
  geom_histogram(binwidth=0.1, bins = 30, fill = "steelblue",  colour="black", position = "dodge") +
  scale_x_continuous(breaks=seq(0, 5, by= 0.5)) +
  labs(title=" Users distribution by average rating ", x=" Average rating ", y=" Number of users ")+ theme(plot.title=element_text(hjust=0.5))

```
Two things stand out as a result of what we can see in the figures:  
  
  • the fact that the ratings of the users to the films can be considered high (their average).  
  • There are user groups that rate thousands of movies and there are those that rate very few movies. These groups can bias our analysis  
  • that the majority of users have given a positive or very positive evaluation of the films they have seen. And there´s a few group of them in the opposite side.  
  
From our point of view, it does not seem that the evaluations made by the users have been correctly carried out (with criteria) or at least substantiated. We do not know if it is part of the process of generating recommendations, thus biasing the information that we are analyzing, or if the users really have a criterion that is sufficiently based on an adequate critical spirit. In any case, we must take both possibilities into account when building prediction models.  
  
### 3.2.3	Genres  
  
The different genres we can find on both data sets are `r validation %>% pull(genres) %>% str_split(pattern="\\|") %>%  unlist() %>% unique() %>% sort() %>%  str_c("*", . , "*") %>%  str_flatten(collapse = ", ")`

However, there's an exception on edx dataset as we can find also entries named as "`r edx %>% pull(genres) %>% str_split(pattern="\\|") %>%  unlist() %>% unique() %>% sort() %>% .[1] %>% str_c("*", . , "*")`".  
  
First, we are going to determine the preferences (via scoring) that users have about the genres of movies they watch:  
```{r Genres, include=FALSE}

genres_dataf <- as.data.frame(genres)
names(genres_dataf) <- c("genre")

genres_dataf$n <- sapply(genres, function(gen) {
  index <- genres_dataf$genre==gen
  nrow(edx[str_detect(edx$genres, gen)])
})

genres_dataf$meanRating <- sapply(genres, function(gen) {
  index <- genres_dataf$genre==gen
  mean(edx[str_detect(edx$genres, gen)]$rating)
})

genres_dataf$sd <- sapply(genres, function(gen) {
  index <- genres_dataf$genre==gen
  sd(edx[str_detect(edx$genres, gen)]$rating)
})
```

```{r Genres_Ratings, include=FALSE}
genres_dataf$se <- genres_dataf$sd / sqrt(genres_dataf$n)
genres_dataf <- genres_dataf %>% arrange(desc(n))

genres_dataf %>% filter(genre!="(no genres listed)") %>%
  mutate(genre = reorder(genre, meanRating)) %>%
  ggplot(aes(x = genre, y = meanRating, ymin=meanRating - 2*se, ymax=meanRating + 2*se)) +
  
  geom_segment( aes(x=genre, xend=genre, y=3, yend=meanRating), color = "gray", lwd = 1) +
  geom_point( size = 4, pch = 21, bg = 4, col = 1)  + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Film genre", y = "Average Rating")

genres_dataf %>% select(genre, n, meanRating)%>%
  kable(col.names = c("Genre", "Ratings Count", "Mean Rating"),
        caption = "Ranked genres by ratings count",
        align = "lrr", booktabs = TRUE, format = "latex", linesep = "")

```
  
Looking at the information provided by the previous figure, we can see that users prefer *Noir-type* movies while at the other end of their preferences are *Horror* movies.  That means that average rating fluctuates significantly. 

### 3.2.4	Time  

The last parameter to finish the data exploration, corresponds to the way the time may affect our predictions. The following figure represent the average ratings of movies by year:
```{r Av_rate_year, echo=FALSE}

edx %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "month")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point(alpha = 0.5) +
  geom_smooth(color = "steelblue", size=1.5) +
  labs(title=" Average of ratings through time ", x=" Year ", y=" Mean rating ")+ theme(plot.title=element_text(hjust=0.5))
```
We may conclude that time, although has some effect on ratings, it is not significant on ratings.

  \newpage
  
# 4.	 Modeling  
  
The evaluation metric utilized in the data modelling of this research is the Root Mean Square Error. As mentioned at the begining of this document, our this project was to achieve a RMSE lower than 0.86490.  

We have defined the following models:  

  • Base Line (Naive). 
  • Movie effect.  
  • User effect.  
  • Regularized Movie and user effect model.  
  • Matrix Factorization (based on the residuals of the best model).  

## 4.1	Model performance  

The way we are going to compare the models, has Root Mean Squared Error (RMSE) as our loss function, as is the standard deviation of the residuals (prediction errors) that are a measure of how far from the regression line data points are. In other words, tells us the average distance between the predicted values from the model and the actual values in the dataset: the lower the RMSE is, the better the model fits.

In the formula shown below, $y_{u,i}$ is defined as the actual rating provided by user $i$ for movie $u$, $\hat{y}_{u,i}$ is the predicted rating for the same, and N is the total number of user/movie combinations.  
$$RMSE = \sqrt{\frac{1}{N}\sum_{u,i}\left(\hat{y}_{u,i}-y_{u,i}\right)^2}$$  

## 4.2	Model criteria

Taking into account the information analyzed throughout this document in relation to the predictor variables, both Genres and Time are not significant as contributors of relevant information. We have splitted into training and testing datasets at 80%:20% ratios respectively. A total of five models have been created in an incremental fashion.  
  
To first build a baseline prediction model, we will consider movie and user effect. The simplest algorithm for predicting ratings is to apply the same rating to all movies. Here, the actual rating for movie $i$ by user $u$, $Y_{u,i}$, is the sum of this "true" rating, $\mu$, plus $\epsilon_{u,i}$, the independent errors sampled for the same distribution.  

$$Y_{u,i}=\mu+\epsilon_{u,i}$$ 

### • Movie effects

We know that movies are rated differently, some higher than others. As there is a high number of movies and ratings, we believe that this point will improve the accuracy of the prediction model: any further improvement to our model, may be to take into account the effect of the movies on the rating $b_i$.  

$$Y_{u,i}=\mu+b_i+\epsilon_{u,i}$$ 

The least squares estimate of the movie effects, $\hat{b}_i$, can be derived from the average of $Y_{u,i}-\hat{\mu}$ for each movie $i$ and, thus, the following formula was used to take account of movie effects:  

$$\hat{b}_{i}=mean\left(\hat{y}_{u,i}-\hat{\mu}\right)$$  

### • Movie and user effects

Of course, some users are more active than others at rating movies, so further refinements have been made to the algorithm in order to adjust for user effects ($b_u$). 
$$Y_{u,i}=\mu+b_i+b_u+\epsilon_{u,i}$$

In this case too, rather than fitting linear regression models, the least square estimates of the user effect, $\hat{b}_u$ has been calculated using:  

$$\hat{b}_{u}=mean\left(\hat{y}_{u,i}-\hat{\mu}-\hat{b}_i\right)$$  

## 4.3 Regularization

Regularization is a form of regression technique that shrinks or regularizes or constraints the coefficient estimates towards 0 (or zero). In this technique, a penalty is added to the various parameters of the model in order to reduce the freedom of the given model. In our case, will allow us to penalize large estimates that come from small sample sizes. 

$$\frac{1}{N}\sum_{u,i}\left(y_{u,i}-\mu-b_i-b_u\right)^2+\lambda\left(\sum_ib_i^2+\sum_ub_u^2\right)$$ 

where the first term $\frac{1}{N}\sum_{u,i}\left(y_{u,i}-\mu-b_i-b_u\right)^2$, strives to find $b_u$’s and $b_i$’s that fit the given ratings. The regularizing term, $\lambda\left(\sum_ib_i^2+\sum_ub_u^2\right)$, avoids over fitting by penalizing the magnitudes of the parameters. This least square problem can be solved via the matrix factorization.

We used cross validation to pick the best $\lambda$ and using calculus we can show that the values of $b_i$ and $b_u$ that minimize this equation are :

$$\hat{b}_i\left(\lambda\right)=\frac{1}{\lambda+n_i}\sum_{u=1}^{n_i}\left(Y_{u,i}-\hat{\mu}\right)$$ 

$$\hat{b}_u\left(\lambda\right)=\frac{1}{\lambda+n_i}\sum_{u=1}^{n_i}\left(Y_{u,i}-\hat{\mu}- \hat{b}_i\right)$$ 

## 4.4 Matrix factorization

Matrix Factorization is a collaborative filtering solution for recommendations. The fundamental assumption behind collaborative filtering technique is that similar user preferences over the items, could be exploited to recommend those items to a user who has not seen or used it before. In simpler terms, we assume that users who agreed in the past (e.g. purchased the same product or viewed the same movie) will agree in the future.

We will apply Matrix Factorization with parallel stochastic gradient descent. In order get the results desired, we have used *Recosystem package* that is typically used to approximate an incomplete matrix using the product of two matrix in a latent space.  
  
\newpage
# 5. Results  
```{r Model1, include=FALSE}

# Create train and test set
set.seed(1, sample.kind="Rounding")
# edx_test set will be 10% of edx data
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)

edx_train <- edx[-test_index,]
edx_temp <- edx[test_index,]

# userId and movieId in test are also in train set
edx_test <- edx_temp %>% 
  semi_join(edx_train, by = "movieId") %>%
  semi_join(edx_train, by = "userId")

# Added rows removed from test set back into train set
removed <- anti_join(edx_temp, edx_test)
edx_train <- rbind(edx_train, removed)

# Define RMSE
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

#Average movie rating
mu <- mean(edx_train$rating)

target_rmse <- 0.86490

# RMSE Model 1
model1_rmse <- RMSE(edx_test$rating, mu)
rmse_results <- data.frame(Model = "Just the Average",
                           RMSE = model1_rmse)

```

## 5.1 Model 1: BaseLine  
\   
Model calculates the average rating. The RMSE related to this model is the one that we will use as a reference to improve successively.\  
  
The average rating is mu = `r round(mu,4)`, and the RMSE is `r round(model1_rmse,4)`.

```{r Result_Table1, echo=FALSE}

rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#737373", bold = T) %>%
  column_spec(2, color =  "#386890", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))
```

## 5.2 Model 2: Movie Effects

It is obvious that the characteristics of the movies affect the ratings provided by users. That induces us to apply b_i for each movie our model.
The average rating for a movie is sure to have a difference from the overall average rating for all movies.

```{r Model2_Mean, echo=FALSE}

# Movie mean rating:  b_i
movie_avgs <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
  
```

```{r Model2_Predict,include=FALSE, echo=FALSE}

# Prediction rating for Model 2
pred_rat_m2 <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  mutate(pred = mu + b_i)

# RMSE Model 2
model2_rmse <- RMSE(edx_test$rating,pred_rat_m2$pred)
rmse_results <- rbind(rmse_results,data.frame(Model = "Movie Effect Model",
                           RMSE = round(model2_rmse, 4)))

```

The estimate of movie effect (b_i) changes significantly over all of the movies included in the train dataset. We can see adding the movie effect into the algorithm improves the accuracy of the model by `r percent((model1_rmse-model2_rmse)/model1_rmse)`, yielding an RMSE of `r round(model2_rmse,4)`, but unfortunately its over our target.

```{r Result_Table2, echo=FALSE}

rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#737373", bold = T) %>%
  column_spec(2, color =  "#386890", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))

```

## 5.3 Model 3: Movies and Users effects

Due the fact users can provide very low ratings for all the movies they have seen, distorting what other users have objectively rated, we will try to reduce that noise by adding the user bias b_u to the movie effect model previously to calculate the RMSE for this model.  

```{r Model3_Mean, echo=FALSE}
 
# User mean rating:  b_u
user_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

```

```{r Model3_Predict, echo=FALSE}

# Prediction rating for Model 3
pred_rat_m3 <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) 

# RMSE Model 3
model3_rmse <- RMSE(edx_test$rating,pred_rat_m3$pred)
rmse_results <- rbind(rmse_results,
                          data_frame(Model="Movie and User Effect model",  
                                    RMSE = round(model3_rmse, 4)))

```

Adjusting for both movie and user effects has allowed us to improve RMSE by `r percent((model1_rmse-model3_rmse)/model1_rmse)` versus the Baseline model.

```{r Result_Table3, echo=FALSE}

rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#737373", bold = T) %>%
  column_spec(2, color =  "#386890", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))

```

## 5.4 Model 4: Movie and user regularization  

In order to avoid overfitted or under fitted, we use regularization to reduce the chance of overfitting and help us get an optimal model. In this case, we will take into account on the movie and user effects, by adding a larger penalty to estimates from smaller samples via the parameter $\lambda$.

The reason is that as we´ve seen before, there are users who have valued a multitude of movies, while others have hardly intervened; In the same way, there are films with many ratings, while others have hardly been rated. This supposes the distortion of the sample data.  

```{r Lambda, echo=FALSE}

# Optimal tuning parameter (Lambda) via k fold cross validation
lambdas <- seq(0, 10, 0.25)

# Best value of lambdas
set.seed(21, sample.kind = "Rounding")

# For each lambda, find b_i & b_u followed by rating prediction
rmses <- sapply(lambdas, function(l){
  
  mu <- mean(edx_train$rating)
  
  b_i <- edx_train %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx_train %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- edx_test %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(edx_test$rating,predicted_ratings))
})

# Plotting lambdas

qplot(lambdas, rmses)+
  geom_rug(col="steelblue",alpha=0.5, size=1.5)+
  geom_point(shape=21, color="black", fill="#4682b4", size=3)+
  labs(x="Lambda", y="RMSE")+
  theme(text = element_text(size=14), plot.caption = element_text(hjust = 0.5, size = 22))

```

```{r Model4, echo=FALSE}

# Optimal value for lambda
lambda <- lambdas[which.min(rmses)]

# Regular movie reg_b_i with optimal lambda
reg_movie_avgs <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(reg_b_i = sum(rating - mu)/(n()+lambda), n_i = n())
  
## Regular user reg_b_u with optimal lambda
reg_user_avgs <- edx_train %>% 
  left_join(reg_movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(reg_b_u = sum(rating - mu - reg_b_i)/(n()+lambda), n_u = n())

# Prediction rating Model 4
reg_predicted_ratings <- edx_test %>% 
  left_join(reg_movie_avgs, by='movieId') %>%
  left_join(reg_user_avgs, by='userId') %>%
  mutate(pred = mu + reg_b_i + reg_b_u) %>% 
  .$pred

# RMSE Model 4
model4_rmse <- RMSE(edx_test$rating,reg_predicted_ratings)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Regularized Movie and User Effect Model",  
                                     RMSE = round(model4_rmse, 4)))

```
  
In the previous figure we can see the RMSE that has been assigned to each of the lambda values that we have tested. The process tells us that the optimal value for $\lambda$ was `r round(lambda,4)` since it reduced the RMSE to `r round(model4_rmse,4)`, which is an improvement of `r percent ((model1_rmse-model4_rmse)/model1_rmse)` in the precision of the baseline and enough to exceed the RMSE that we had set as the project objective.  

```{r Result_Table4, echo=FALSE}

rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#737373", bold = T) %>%
  column_spec(2, color =  "#386890", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))
```

## 5.5 Model 5: Matrix Factorization  
  
```{r Model5, include=FALSE}

# Model 5: Matrix Factorization

# Residuals of edx_train from Model 4
residual_edx <- edx_train %>% 
  left_join(reg_movie_avgs, by = "movieId") %>%
  left_join(reg_user_avgs, by = "userId") %>%
  mutate(residual = rating - mu - reg_b_i - reg_b_u) %>%
  select(userId, movieId, residual)

# Matrix from residual and edx_test
residual_mf <- as.matrix(residual_edx)
edx_test_mf <- edx_test %>% 
  select(userId, movieId, rating)
edx_test_mf <- as.matrix(edx_test_mf)

# Residual_mf and edx_test_mf to hard disk
write.table(residual_mf , file = "trainset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)
write.table(edx_test_mf, file = "testset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)

# data_file() to specify a data set from a file at hard disk.
train_set <- data_file("trainset.txt")
test_set <- data_file("testset.txt")

## Build a recommender object
r <-Reco()

# Tuning training set
# Note: following code can take up to 20 minutes.
opts <- r$tune(train_set, opts = list(dim = c(10, 20, 30), lrate = c(0.1, 0.2),
                                      costp_l1 = 0, costq_l1 = 0,
                                      nthread = 1, niter = 10))

# Training recommender model
r$train(train_set, opts = c(opts$min, nthread = 1, niter = 20))

# Making prediction on validation set and calculating RMSE:
pred_file <- tempfile()
r$predict(test_set, out_file(pred_file)) 

predicted_residuals_mf <- scan(pred_file)
predicted_ratings_mf <- reg_predicted_ratings + predicted_residuals_mf

# RMSE for Model 5
model5_rmse <- RMSE(edx_test$rating, predicted_ratings_mf)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Matrix Factorization",  
                                     RMSE = round(model5_rmse, 4)))

```
Applying the Matrix factorization method, we obtain an RMSE equal to `r round(model5_rmse,4)` This represents a reduction of
 `r percent ((model1_rmse-model5_rmse)/model1_rmse)` relative to the baseline model.

```{r Result_Table5, echo=FALSE}

rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#737373", bold = T) %>%
  column_spec(2, color =  "#386890", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))
```

```{r Final_Model, echo=FALSE}

# Optimal Lambda via k fold cross validation
set.seed(1, sample.kind = "Rounding")

# Average movie rating for edx data set
f_mu <- mean(edx$rating)

# Best value of lambdas returning min RMSE
f_lambdas <- seq(0, 10, 0.25)

f_rmses <- sapply(f_lambdas, function(l){
  
  mu <- mean(edx$rating)
  
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(validation$rating,predicted_ratings))
})

# Optimal value for lambda
f_lambda <- f_lambdas[which.min(f_rmses)]
f_lambda
qplot(f_lambdas, f_rmses)+
  geom_rug(col="steelblue",alpha=0.5, size=1.5)+
  geom_point(shape=21, color="black", fill="#4682b4", size=3)+
  labs(x="Lambda", y="RMSE")+
  theme(text = element_text(size=14), plot.caption = element_text(hjust = 0.5, size = 22))
```

```{r Final_Model1, echo=FALSE}

# Final regular movie f_b_i with optimal lambda
final_movie_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(f_b_i = sum(rating - f_mu)/(n()+f_lambda), n_i = n())

# Final regular user f_b_u with optimal lambda
final_user_avgs <- edx %>% 
  left_join(final_movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(f_b_u = sum(rating - f_mu - f_b_i)/(n()+f_lambda), n_u = n())

# Regular prediction rating
final_reg_predicted_ratings <- validation %>% 
  left_join(final_movie_avgs, by='movieId') %>%
  left_join(final_user_avgs, by='userId') %>%
  mutate(pred = f_mu + f_b_i + f_b_u) %>% 
  .$pred

# Residuals of edx data set
final_residual_edx <- edx %>% 
  left_join(final_movie_avgs, by = "movieId") %>%
  left_join(final_user_avgs, by = "userId") %>%
  mutate(residual = rating - f_mu - f_b_i - f_b_u) %>%
  select(userId, movieId, residual)

# Matrix from residual and validation set
final_residual_mf <- as.matrix(final_residual_edx)
validation_mf <- validation %>% 
  select(userId, movieId, rating)
validation_mf <- as.matrix(validation_mf)

# final_residual_mf and validation_mf to hard disk
write.table(final_residual_mf , file = "final_trainset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)
write.table(validation_mf, file = "final_testset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)

# data_file() to specify a data set from a file to hard disk.
final_train_set <- data_file("final_trainset.txt")
final_test_set <- data_file("final_testset.txt")

# Build a recommender object
f_r <-Reco()

# Tuning training set
# Note: following code can take up to 20 minutes.
f_opts <- f_r$tune(final_train_set, opts = list(dim = c(10, 20, 30), lrate = c(0.1, 0.2),
                                      costp_l1 = 0, costq_l1 = 0,
                                      nthread = 1, niter = 10))

# Training recommended model
f_r$train(final_train_set, opts = c(f_opts$min, nthread = 1, niter = 20))

# Making prediction on validation set and calculating RMSE:
final_pred_file <- tempfile()
f_r$predict(final_test_set, out_file(final_pred_file)) 

final_predicted_residuals_mf <- scan(final_pred_file)
final_predicted_ratings_mf <- final_reg_predicted_ratings + final_predicted_residuals_mf

# RMSE for final model (Model 5: Matrix Factorization)
final_rmse <- RMSE(validation$rating, final_predicted_ratings_mf)
final_rmse_results <- data.frame(Model = "Best Model: Matrix Factorization",
                                 RMSE = round(final_rmse, 4))

```

## 5.6 Final Model  
  
As we are able to see in the previous sections, we´ve been building the models to be evaluated through edx_test dataset. The best performing model for this subset is Matrix Factorization which produces an RMSE of `r round(model5_rmse,4)`.

Bearing this result in mind, we consider the final model must adopt the same methodology being the difference it has to be constructed over the entire data set, but expecting at least the same results, if not better ones.

After running the model, the final test on the validation dataset achieves an RMSE of `r round(final_rmse,4)`, an improvement of `r percent((model1_rmse-final_rmse)/model1_rmse)` compared to the baseline model:  

```{r Final_validation, echo=FALSE}
final_rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#737373", bold = T) %>%
  column_spec(2, color =  "#386890", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))
```

\newpage

# 6. Recapitulation 

In this project, a recommender system was created for the 10 million Movielens dataset with a RMSE of 0.86396 when tested on a hold-out dataset, which is
10% of the original dataset. 

Through five different models developed, we have reached a final model, Matrix Factorization, which yields an RMSE of `r round(final_rmse,4)` when trained on edx and tested on validation being an improvement over the first model of a `r percent((model1_rmse-final_rmse)/model1_rmse)`.















